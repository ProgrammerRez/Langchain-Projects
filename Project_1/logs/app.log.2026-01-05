2026-01-05 23:15:38,673 - INFO - project_1 - ðŸš€ Starting server at http://127.0.0.1:5000
2026-01-05 23:16:11,512 - INFO - project_1 - ðŸš€ Starting server at http://127.0.0.1:5000
2026-01-05 23:16:31,510 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:03,205 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:16,102 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:46,454 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:49,607 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:51,220 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:17:53,034 - INFO - project_1 - ðŸ“„ Document classified | id=doc_001 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:18:14,668 - INFO - project_1 - ðŸ“„ Document classified | id=doc_1 | type=invoice | confidence=0.900 | ambiguous=False
2026-01-05 23:18:19,734 - INFO - project_1 - ðŸ“„ Document classified | id=doc_3 | type=medical_record | confidence=0.900 | ambiguous=False
2026-01-05 23:20:18,511 - INFO - project_1 - ðŸ“„ Document classified | id=doc_2 | type=purchase_order | confidence=0.900 | ambiguous=False
2026-01-05 23:20:24,915 - ERROR - project_1 - ðŸ”¥ Unexpected server error
Traceback (most recent call last):
  File "D:\Side-Projects\Langchain Projects\Project_1\demo.py", line 88, in classify_document
    validation = validate_document(
                 ^^^^^^^^^^^^^^^^^^
  File "D:\Side-Projects\Langchain Projects\Project_1\steps\Validation.py", line 93, in validate_document
    result: DocumentValidation = chain.invoke({
                                 ^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\runnables\base.py", line 3151, in invoke
    input_ = context.run(step.invoke, input_, config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\runnables\base.py", line 5557, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in invoke
    self.generate_prompt(
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1117, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\language_models\chat_models.py", line 927, in generate
    self._generate_with_cache(
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1221, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\langchain_groq\chat_models.py", line 593, in _generate
    response = self.client.create(messages=message_dicts, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\groq\resources\chat\completions.py", line 461, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\groq\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\DELL\.conda\envs\langchain-portfolio\Lib\site-packages\groq\_base_client.py", line 1044, in request
    raise self._make_status_error_from_response(err.response) from None
groq.APIStatusError: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01jzf6z41ke59rggmn4z3nfgb6` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 20932, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
